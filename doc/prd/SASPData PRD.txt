Here are some actual URLs

https://virtual.sssfonline.com/api/shot/sasp-scoreboard/624
https://virtual.sssfonline.com/api/shot/sasp-schedule/624
https://virtual.sssfonline.com/api/teams/1894
https://virtual.sssfonline.com/api/shot/SASP/competitions?type=S&page=1




heck yes—this is a great moment to de‑Fabric and stand up a lean Windows 10 + Postgres pipeline. Below is a practical PRD (product requirements doc) tailored to what you uploaded and where you want to go, followed by a concrete reference implementation you can drop onto your Win10 box. I’ve designed it so you can start by ingesting raw JSON straight into Postgres (jsonb), then transform to the “silver” and “gold” relational tables you’re used to—without the Spark + Delta overhead.

# PRD — SASP ETL on Windows 10 + Postgres

## 1) Problem & Goals

You currently run PySpark/Fabric pipelines that ingest scoreboard/schedule/team JSON, land into Delta “bronze,” transform into “silver,” and aggregate to “gold” (e.g., fMatch, fStage, fString, fMasters). You want:

* A self‑contained Windows 10 server solution, no Fabric dependencies.
* Postgres as the single data store.
* Faster, simpler ingest with version tracking.
* Clear lineage from raw JSON ➜ relational tables ➜ analytics-ready facts.
* Maintain (and improve) your existing logic (e.g., unpivoting spp\_\* columns, stage-name retention, fMasters best-of Rifle/Pistol/Revolver).

### Success criteria

* **Reliability:** Idempotent ingest, upsert semantics, no dupes.
* **Observability:** Row counts and run logs per job step.
* **Performance:** JSON lands “as-is” in seconds; transforms run in SQL close to the data.
* **Portability:** Pure Python + psql. No Fabric/Spark/Delta needed.
* **Reviewability:** Versioned raw payloads with source\_hash and timestamps.

## 2) Non-goals

* Orchestrators like Airflow/Prefect (nice-to-have later). We’ll use Windows Task Scheduler now.
* Real-time streaming. This is a batch pull on a schedule.

## 3) Data Sources (based on your notebooks)

* **Scoreboard JSON**
* **Schedule JSON**
* **Teams & Positions JSON**
* (Derived) **fmatch** logic, **defDiscipline** mapping, **fMasters** aggregation (your uploaded `fMaster (1).json` shows the M script and intended outputs).

## 4) Target Architecture

* **Ingestion (Bronze/raw):** Python `requests` pulls JSON and writes directly into Postgres `raw_*` tables as `jsonb` rows with metadata (ingested\_at, source, source\_hash, match\_number when available).
* **Normalization (Silver):** SQL in Postgres extracts from `jsonb` to typed relational tables (`silver_scoreboard`, `silver_schedule`, `silver_teams`, …). Handle unpivoting, stage-name retention, and dedup at this layer.
* **Facts/Dimensions (Gold):** SQL views/materialized tables for `fmatch`, `fstage`, `fstring`, `fmasters`, and dimensions (e.g., `dimdiscipline`).
* **Orchestration:** Simple Python entrypoint that runs steps in order; scheduled with Windows Task Scheduler.
* **Env:** Python 3.11+, `psycopg2-binary` or `asyncpg`, `requests`, `pydantic` (optional), `SQLAlchemy` (optional), and Postgres 14+.

## 5) Schemas

### 5.1 Bronze / Raw (append-only, versioned)

* `raw_scoreboard`

  * `id` bigserial PK
  * `match_number` int NULL
  * `payload` jsonb NOT NULL
  * `source` text NOT NULL
  * `source_hash` text NOT NULL UNIQUE
  * `ingested_at` timestamptz DEFAULT now()

* `raw_schedule` (same columns as above)

* `raw_teams` (same columns as above)

> `source_hash` = sha256 of canonicalized JSON to avoid duplicate inserts across runs.

### 5.2 Silver / Normalized

(Representative—adjust columns to mirror your current model.)

* `silver_schedule` (deduped, Station dropped if needed)

  * `match_number` int
  * `match_name` text
  * `match_squad_id` int
  * `squad_name` text
  * `squad_number` int
  * `discipline` text
  * `flight_date` date
  * `flight_time` timestamptz NULL
  * `location` text
  * `division` text
  * `full_name` text  -- synthesized `first + ' ' + last` for joins
  * `updated_at` timestamptz

* `silver_scoreboard`

  * `match_number` int
  * `team_id` int
  * `athlete_id` int
  * `sex` text
  * `discipline` text
  * `discipline_id` int
  * `stage` text
  * `string` int
  * `stage_completed` text
  * `stage_raw_time` numeric
  * `stage_missed_plates` int
  * `net_time` numeric        -- with your “drop-slowest” rule applied later or stored separately
  * `updated_at` timestamptz

* `silver_teams` / `silver_positions` — as needed for joins.

> **Stage-name retention:** when the source already provides stage names, keep them; else use alias mapping. We’ll implement this in SQL with `COALESCE(existing_stage, alias_map.stage)`.

### 5.3 Gold / Facts & Dimensions

* `dim_discipline` (static inline plus id mapping)

  * `discipline` text PK
  * `discipline_id` int
  * `type` text  -- Rifle/Pistol/Revolver (from your `defDiscipline`)

* `fmatch` (per athlete/discipline/match with match-level rollups used by your M script)

  * `match_number` int
  * `team_id` int
  * `athlete_id` int
  * `discipline_id` int
  * `class_id` int
  * `sex` text
  * `match_net_time` numeric
  * `stage_count` int
  * `match_complete` text
  * `match_rank` int

* `fmasters` (best Rifle + best Pistol + best Revolver per match/team/athlete)

  * `match_number` int
  * `team_id` int
  * `athlete_id` int
  * `rifle_best` numeric
  * `pistol_best` numeric
  * `revolver_best` numeric
  * `master_score` numeric
  * `team_ath_id` text  -- “TeamID-AthleteID”
  * `rank_in_match_team` int
  * `class_id` int

> This mirrors the logic in your **fMaster M**: filter to Type ∈ {Rifle,Pistol,Revolver}, find best MatchNetTime per gun, sum to MasterScore, then rank.

## 6) Pipeline Steps

1. **Ingest (raw JSON → Postgres jsonb)**

* Pull JSON endpoints/files.
* Create `source_hash`; INSERT … ON CONFLICT DO NOTHING.

2. **Silver Normalize (jsonb → relational)**

* Use `jsonb_to_recordset` and `->>` extraction to typed columns.
* Apply business rules:

  * Stage-name retention via `COALESCE`.
  * Unpivoting spp\_\* (if applicable) using SQL UNNEST or set-returning functions.
  * Deduping (e.g., `DISTINCT ON (...) ORDER BY updated_at DESC`).

3. **Gold build**

* `fmatch` materialized via SQL from `silver_scoreboard` + joins.
* `fmasters` (gun “Type” join via `dim_discipline`), “best-of” per gun, sum to `master_score`, rank within (match, team).

4. **Scheduling**

* A single Python CLI script with subcommands `ingest`, `silver`, `gold`, `all`.
* Windows Task Scheduler runs `all` nightly (and `ingest` more frequently if desired).

## 7) Tradeoffs & the “Direct-to-DB JSON” question

**Yes—ingesting JSON directly into Postgres first is what I’d recommend.** Why:

* **Speed:** network + single INSERT (COPY) into one place beats Python manipulating a bunch of files first.
* **Versioning:** immutable raw rows keep perfect provenance (who/what/when from `source_hash`, `ingested_at`).
* **Observability:** counts and hashes in SQL; easy to diff between runs.
* **Flexibility:** you can iterate transformations in SQL without re-pulling the source.

The only “con” is you’re moving transform logic into SQL instead of PySpark—but Postgres handles jsonb and set-based transforms extremely well, and you gain simplicity.

---

# Reference Implementation (full, runnable)

Below is end-to-end code: DDL, Python ingest, SQL transforms, and a simple “orchestrator.” It’s intentionally explicit, so you can paste and go.

## A) Postgres DDL

```sql
-- 0) Enable useful extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- 1) Raw tables
CREATE TABLE IF NOT EXISTS raw_scoreboard (
  id           bigserial PRIMARY KEY,
  match_number int NULL,
  payload      jsonb NOT NULL,
  source       text  NOT NULL,
  source_hash  text  NOT NULL UNIQUE,
  ingested_at  timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE IF NOT EXISTS raw_schedule (
  id           bigserial PRIMARY KEY,
  match_number int NULL,
  payload      jsonb NOT NULL,
  source       text  NOT NULL,
  source_hash  text  NOT NULL UNIQUE,
  ingested_at  timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE IF NOT EXISTS raw_teams (
  id           bigserial PRIMARY KEY,
  match_number int NULL,
  payload      jsonb NOT NULL,
  source       text  NOT NULL,
  source_hash  text  NOT NULL UNIQUE,
  ingested_at  timestamptz NOT NULL DEFAULT now()
);

-- 2) DimDiscipline (inline mapping; adjust as in your Power Query)
CREATE TABLE IF NOT EXISTS dim_discipline (
  discipline     text PRIMARY KEY,
  gun            text,
  sights         text,
  ord            int,
  type           text,   -- Rifle/Pistol/Revolver
  ammo           text,
  discipline_id  int UNIQUE
);

-- 3) Silver tables (normalized)
CREATE TABLE IF NOT EXISTS silver_schedule (
  match_number   int,
  match_name     text,
  match_squad_id int,
  squad_name     text,
  squad_number   int,
  discipline     text,
  flight_date    date,
  flight_time    timestamptz,
  location       text,
  division       text,
  full_name      text,
  updated_at     timestamptz,
  PRIMARY KEY (match_number, match_squad_id, full_name)
);

CREATE TABLE IF NOT EXISTS silver_scoreboard (
  match_number       int,
  team_id            int,
  athlete_id         int,
  sex                text,
  discipline         text,
  discipline_id      int,
  stage              text,
  string             int,
  stage_completed    text,
  stage_raw_time     numeric,
  stage_missed       int,
  net_time           numeric,
  updated_at         timestamptz,
  PRIMARY KEY (match_number, team_id, athlete_id, discipline_id, stage, string)
);

-- 4) Gold (facts)
CREATE TABLE IF NOT EXISTS fmatch (
  match_number   int,
  team_id        int,
  athlete_id     int,
  discipline_id  int,
  class_id       int,
  sex            text,
  match_net_time numeric,
  stage_count    int,
  match_complete text,
  match_rank     int,
  PRIMARY KEY (match_number, team_id, athlete_id, discipline_id)
);

CREATE TABLE IF NOT EXISTS fmasters (
  match_number         int,
  team_id              int,
  athlete_id           int,
  rifle_best           numeric,
  pistol_best          numeric,
  revolver_best        numeric,
  master_score         numeric,
  team_ath_id          text,
  rank_in_match_team   int,
  class_id             int,
  PRIMARY KEY (match_number, team_id, athlete_id)
);
```

## B) Python — Ingest raw JSON straight to Postgres

> Configure via env: `PG_DSN` (e.g., `postgresql://user:pass@localhost:5432/sasp`).

```python
# file: etl_ingest.py
import os, json, hashlib, datetime as dt
import psycopg2
import psycopg2.extras
import requests

PG_DSN = os.getenv("PG_DSN", "postgresql://postgres:postgres@localhost:5432/sasp")

def _sha256(obj) -> str:
    # stable hash of JSON value
    s = json.dumps(obj, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def _insert_raw(rows, table):
    with psycopg2.connect(PG_DSN) as cx:
        with cx.cursor() as cur:
            sql = f"""
                INSERT INTO {table} (match_number, payload, source, source_hash, ingested_at)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (source_hash) DO NOTHING
            """
            psycopg2.extras.execute_batch(cur, sql, rows, page_size=1000)

def ingest_from_url(url: str, table: str, source_name: str, match_number: int | None):
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    data = r.json()

    # if top-level is a collection of records
    records = data if isinstance(data, list) else [data]
    now = dt.datetime.utcnow()

    batch = []
    for rec in records:
        h = _sha256(rec)
        batch.append((match_number, json.dumps(rec), source_name, h, now))

    _insert_raw(batch, table)
    return len(batch)

def ingest_from_file(path: str, table: str, source_name: str, match_number: int | None):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    records = data if isinstance(data, list) else [data]
    now = dt.datetime.utcnow()
    batch = []
    for rec in records:
        h = _sha256(rec)
        batch.append((match_number, json.dumps(rec), source_name, h, now))
    _insert_raw(batch, table)
    return len(batch)

if __name__ == "__main__":
    # Examples (switch these to your real endpoints/files)
    # URLs:
    # ingest_from_url("https://api.example/scoreboard?match=730", "raw_scoreboard", "scoreboard_api", 730)
    # ingest_from_url("https://api.example/schedule?match=730",  "raw_schedule",  "schedule_api", 730)

    # Files (local drop):
    # scoreboard_count = ingest_from_file(r"C:\ingest\scoreboard_730.json", "raw_scoreboard", "file:scoreboard_730", 730)
    # schedule_count   = ingest_from_file(r"C:\ingest\schedule_730.json",   "raw_schedule",  "file:schedule_730", 730)
    pass
```

## C) SQL — Silver normalization (jsonb ➜ typed tables)

> Adjust JSON paths to match your payloads. The patterns below reflect your Fabric logic (names like `MatchNumber`, `MatchName`, `SquadID`, etc.) from the notebooks and prior chats.

```sql
-- Example: schedule normalization
WITH src AS (
  SELECT r.match_number,
         r.payload
  FROM raw_schedule r
)
, flat AS (
  -- If payload is an array of rows:
  SELECT
    (payload->>'MatchNumber')::int                      AS match_number,
    payload->>'MatchName'                               AS match_name,
    (payload->>'SquadID')::int                          AS match_squad_id,
    payload->>'SquadName'                               AS squad_name,
    (payload->>'SquadNumber')::int                      AS squad_number,
    payload->>'Discipline'                              AS discipline,
    (payload->>'FlightDate')::date                      AS flight_date,
    NULLIF(payload->>'FlightTime','')::timestamptz      AS flight_time,
    payload->>'Location'                                AS location,
    payload->>'Division'                                AS division,
    trim(both ' ' from concat(payload->>'AthleteFirstName',' ',payload->>'AthleteLastName')) AS full_name,
    now()                                               AS updated_at
  FROM src
)
INSERT INTO silver_schedule AS t
SELECT DISTINCT * FROM flat
ON CONFLICT (match_number, match_squad_id, full_name)
DO UPDATE SET
  match_name   = EXCLUDED.match_name,
  squad_name   = EXCLUDED.squad_name,
  squad_number = EXCLUDED.squad_number,
  discipline   = EXCLUDED.discipline,
  flight_date  = EXCLUDED.flight_date,
  flight_time  = EXCLUDED.flight_time,
  location     = EXCLUDED.location,
  division     = EXCLUDED.division,
  updated_at   = now();

-- Example: scoreboard normalization (includes stage-name retention)
WITH src AS (
  SELECT r.match_number, r.payload
  FROM raw_scoreboard r
)
, flat AS (
  SELECT
    (payload->>'MatchNumber')::int             AS match_number,
    (payload->>'TeamID')::int                  AS team_id,
    (payload->>'AthleteID')::int               AS athlete_id,
    payload->>'Sex'                             AS sex,
    payload->>'Discipline'                      AS discipline,
    (payload->>'DisciplineID')::int             AS discipline_id,
    -- Prefer explicit stage name if present; else derive from alias fields
    COALESCE(NULLIF(payload->>'Stage',''),
             NULLIF(payload->>'StageName',''),
             'Stage ' || COALESCE(payload->>'StageID','?')) AS stage,
    NULLIF(payload->>'String','')::int          AS string,
    payload->>'StageCompleted'                  AS stage_completed,
    NULLIF(payload->>'StageRawTime','')::numeric       AS stage_raw_time,
    NULLIF(payload->>'StageMissedPlates','')::int      AS stage_missed,
    NULLIF(payload->>'NetTime','')::numeric            AS net_time,
    now() AS updated_at
  FROM src
)
INSERT INTO silver_scoreboard AS t
SELECT DISTINCT * FROM flat
WHERE match_number IS NOT NULL
ON CONFLICT (match_number, team_id, athlete_id, discipline_id, stage, string)
DO UPDATE SET
  sex             = EXCLUDED.sex,
  discipline      = EXCLUDED.discipline,
  stage_completed = EXCLUDED.stage_completed,
  stage_raw_time  = EXCLUDED.stage_raw_time,
  stage_missed    = EXCLUDED.stage_missed,
  net_time        = EXCLUDED.net_time,
  updated_at      = now();
```

## D) SQL — Gold (`fmatch`, then `fmasters`)

```sql
-- Populate / refresh dim_discipline first (inline list you use in Power Query)
-- e.g. INSERT INTO dim_discipline (discipline, gun, sights, ord, type, ammo, discipline_id) VALUES ...;

-- fmatch: roll up scoreboard rows into match-level time per athlete+discipline
-- NOTE: adapt “drop slowest string” rule; below shows plain MIN/SUM pattern placeholders.
WITH per_stage AS (
  SELECT
    match_number, team_id, athlete_id, discipline_id, stage,
    MIN(net_time)                AS best_string_net_time,
    COUNT(*)                     AS strings_fired
  FROM silver_scoreboard
  GROUP BY 1,2,3,4,5
),
per_discipline AS (
  SELECT
    match_number, team_id, athlete_id, discipline_id,
    SUM(best_string_net_time)    AS match_net_time,
    COUNT(*)                     AS stage_count
  FROM per_stage
  GROUP BY 1,2,3,4
),
ranked AS (
  SELECT pd.*,
         -- mark as complete if stage_count >= expected (tune to your league rules)
         CASE WHEN stage_count >= 4 THEN 'Complete' ELSE 'Partial' END AS match_complete,
         RANK() OVER (PARTITION BY match_number, discipline_id ORDER BY match_net_time ASC) AS match_rank
  FROM per_discipline pd
)
INSERT INTO fmatch AS t
SELECT
  r.match_number, r.team_id, r.athlete_id, r.discipline_id,
  /* class_id */ NULL::int AS class_id,
  /* sex */ NULL::text     AS sex,
  r.match_net_time, r.stage_count, r.match_complete, r.match_rank
FROM ranked r
ON CONFLICT (match_number, team_id, athlete_id, discipline_id)
DO UPDATE SET
  match_net_time  = EXCLUDED.match_net_time,
  stage_count     = EXCLUDED.stage_count,
  match_complete  = EXCLUDED.match_complete,
  match_rank      = EXCLUDED.match_rank;

-- fmasters: best Rifle/Pistol/Revolver per athlete+team+match, then sum
WITH typed AS (
  SELECT fm.*,
         dd.type AS gun_type
  FROM fmatch fm
  JOIN dim_discipline dd USING (discipline_id)
  WHERE dd.type IN ('Rifle','Pistol','Revolver') AND fm.match_complete = 'Complete'
),
best_per_type AS (
  SELECT
    match_number, team_id, athlete_id, gun_type,
    MIN(match_net_time) AS best_time
  FROM typed
  GROUP BY 1,2,3,4
),
pivoted AS (
  SELECT
    match_number, team_id, athlete_id,
    MAX(CASE WHEN gun_type='Rifle'    THEN best_time END) AS rifle_best,
    MAX(CASE WHEN gun_type='Pistol'   THEN best_time END) AS pistol_best,
    MAX(CASE WHEN gun_type='Revolver' THEN best_time END) AS revolver_best
  FROM best_per_type
  GROUP BY 1,2,3
),
scored AS (
  SELECT
    p.*,
    (rifle_best + pistol_best + revolver_best) AS master_score,
    (team_id::text || '-' || athlete_id::text) AS team_ath_id
  FROM pivoted p
  WHERE rifle_best IS NOT NULL AND pistol_best IS NOT NULL AND revolver_best IS NOT NULL
),
ranked AS (
  SELECT s.*,
         RANK() OVER (PARTITION BY match_number, team_id ORDER BY master_score ASC) AS rank_in_match_team
  FROM scored s
)
INSERT INTO fmasters AS t
SELECT
  match_number, team_id, athlete_id,
  rifle_best, pistol_best, revolver_best, master_score, team_ath_id,
  rank_in_match_team,
  NULL::int AS class_id
FROM ranked
ON CONFLICT (match_number, team_id, athlete_id)
DO UPDATE SET
  rifle_best         = EXCLUDED.rifle_best,
  pistol_best        = EXCLUDED.pistol_best,
  revolver_best      = EXCLUDED.revolver_best,
  master_score       = EXCLUDED.master_score,
  team_ath_id        = EXCLUDED.team_ath_id,
  rank_in_match_team = EXCLUDED.rank_in_match_team;
```

## E) Simple Orchestrator (Windows-friendly)

```python
# file: etl_cli.py
import argparse, subprocess, sys
from etl_ingest import ingest_from_file, ingest_from_url

def run_sql(path):
    # Requires psql on PATH; or execute via psycopg2 if you prefer.
    cmd = ["psql", "-v", "ON_ERROR_STOP=1", "-f", path]
    return subprocess.call(cmd)

def cmd_ingest(args):
    # wire up your sources here
    # Example: local files dropped by another process
    # ingest_from_file(r"C:\ingest\scoreboard_730.json", "raw_scoreboard", "file:scoreboard_730", 730)
    print("Ingest complete.")

def cmd_silver(args):
    rc1 = run_sql(r"C:\etl\sql\silver_schedule.sql")
    rc2 = run_sql(r"C:\etl\sql\silver_scoreboard.sql")
    if rc1 or rc2:
        sys.exit(1)

def cmd_gold(args):
    rc1 = run_sql(r"C:\etl\sql\gold_fmatch.sql")
    rc2 = run_sql(r"C:\etl\sql\gold_fmasters.sql")
    if rc1 or rc2:
        sys.exit(1)

def cmd_all(args):
    cmd_ingest(args)
    cmd_silver(args)
    cmd_gold(args)

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    sp = ap.add_subparsers()

    p1 = sp.add_parser("ingest"); p1.set_defaults(func=cmd_ingest)
    p2 = sp.add_parser("silver"); p2.set_defaults(func=cmd_silver)
    p3 = sp.add_parser("gold");   p3.set_defaults(func=cmd_gold)
    p4 = sp.add_parser("all");    p4.set_defaults(func=cmd_all)

    args = ap.parse_args()
    if hasattr(args, "func"):
        args.func(args)
    else:
        ap.print_help()
```

---

## Notes I inferred from your uploaded `fMaster` definition and recent work

* Your **fMasters** logic in Fabric Power Query:

  * Joins `fmatch` to `defDiscipline` to get `Type`.
  * Filters to `Type ∈ {Rifle, Pistol, Revolver}` and `MatchComplete = "Complete"`.
  * Picks **best (lowest)** `MatchNetTime` per gun type.
  * Sums to `MasterScore`, then ranks within `(MatchNumber, TeamID)`.
* You also rely on a `TeamAthID` (concatenation of `TeamID` and `AthleteID`) across tables.
* Stage-name retention has bitten before; the silver step above uses `COALESCE` to prefer provided names.

If you want to keep some transforms in Python (for parity with old PySpark code), cool—but Postgres SQL against jsonb is fast and keeps your logic declarative.

---